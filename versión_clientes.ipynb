{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwZacMWBtdc4"
      },
      "outputs": [],
      "source": [
        "#Librería para manipular datos\n",
        "import pandas as pd\n",
        "# llamar a la API de Zendesk\n",
        "import numpy as np\n",
        "import httplib2\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import base64\n",
        "import glob\n",
        "import os\n",
        "from os import walk\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#config\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrc7LvrsnIbL"
      },
      "outputs": [],
      "source": [
        "#_______________________________________________________________________________________\n",
        "z_domain=\"\"\n",
        "email=\"\"\n",
        "contraseña=\"\"\n",
        "start_time=\"946684800\"\n",
        "#start_time=\"1651363200\"\n",
        "#start_time=\"1660660764\"\n",
        "carpeta_contenedora=\"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "#_______________________________________________________________________________________\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nIepn0YWlkuk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDuiHaWItfGZ"
      },
      "outputs": [],
      "source": [
        "authentication=email+\"/token:\"+contraseña\n",
        "message_bytes = authentication.encode('ascii')\n",
        "base64_bytes=base64.b64encode(message_bytes).decode('ascii')\n",
        "headers = {'Authorization': 'Basic '+base64_bytes, 'Content-Type': 'application/json'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_U-TqhSPQuf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_ticket(z_domain,start_time,headers):\n",
        "  url = z_domain+\"/api/v2/incremental/tickets.json?start_time=\"   +start_time+   \"&include=metric_sets,organizations,users\"\n",
        "  status = 0\n",
        "  while status != 200:\n",
        "    r = requests.get(url, headers=headers)\n",
        "    \n",
        "    #response, content = http.request(url, 'GET', headers=headers)\n",
        "    status = r.status_code\n",
        "    if status == 200:\n",
        "      print(\"----llamada de api exitosa: \", status)\n",
        "      print(\"----respuesta: \", r.text)\n",
        "    else:\n",
        "      print(\"----llamada de api fracasada: \", status)\n",
        "      print(\"----respuesta: \", r.text)\n",
        "      time.sleep(20)\n",
        "\n",
        "  data=json.loads(r.text)\n",
        "  is_cont=data['end_of_stream']\n",
        "  end_time=data['end_time']\n",
        "\n",
        "  df_ticket=pd.DataFrame(data['tickets'])\n",
        "\n",
        "  all_cf=pd.DataFrame()\n",
        "  # Se iteran todos los valores de cada ticket devuelto, pueden ser hasta 1000\n",
        "  for idx, x in enumerate(df_ticket['id']):\n",
        "    #Purga e instancia estos arreglos\n",
        "    custom_v_col=[]\n",
        "    custom_v_val=[]\n",
        "    #Purga e instancia el dataset\n",
        "    df_cf=pd.DataFrame()\n",
        "    c_val=df_ticket['custom_fields'][idx]\n",
        "    for idc, c in enumerate(c_val):\n",
        "      #Registra en 2 arreglos, los ID que serán usandos como columnas y los valores como elementos de cada fila\n",
        "      custom_v_col.append(str(c['id']))\n",
        "      custom_v_val.append(str(c['value']))\n",
        "    # Crea la fila del ticket\n",
        "    df_cf=pd.DataFrame([custom_v_val], columns=custom_v_col)\n",
        "    # Agrega el canal por donde llego el ticket\n",
        "    df_cf['canal ingreso']=df_ticket['via'][idx]['channel']\n",
        "    # concatena las nueva filas con otras creadas anteriormente\n",
        "    all_cf = pd.concat([all_cf, df_cf])\n",
        "    \n",
        "\n",
        "  #Reinicia el index del arreglo\n",
        "  all_cf=all_cf.reset_index()\n",
        "  # se unen los 2 dataset\n",
        "  df_ticket = pd.concat([df_ticket, all_cf], axis=1)\n",
        "\n",
        "  #se traspasa las variables del satisfaction_rating\n",
        "  score=[]\n",
        "  comment=[]\n",
        "  reason=[]\n",
        "  for i in df_ticket['satisfaction_rating']:\n",
        "    \n",
        "   if i['score'] == \"unoffered\":\n",
        "     score.append(\"No se ha ofrecido\")\n",
        "   elif i['score'] == \"offered\":\n",
        "     score.append(\"Se ha ofrecido\")\n",
        "   elif i['score'] == \"good\":\n",
        "     score.append(i['score'])\n",
        "   elif i['score'] == \"bad\":\n",
        "     score.append(i['score'])\n",
        "\n",
        "   try:\n",
        "     comment.append(i['comment'])\n",
        "   except:\n",
        "     comment.append(\"no hay comentarios\")\n",
        "   try:\n",
        "     reason.append(i['comment'])\n",
        "   except:\n",
        "     reason.append(\"No se proporcionó una razón\")\n",
        "  df_ticket['indice de satisfaccion']=score\n",
        "  df_ticket['comentario satisfaccion']=comment\n",
        "  df_ticket['motivos satisfaccion']=comment\n",
        "\n",
        "  #se traspasa las variables del metric_set\n",
        "  group_stations=[]\n",
        "  assignee_stations=[]\n",
        "  reopens=[]\n",
        "  replies=[]\n",
        "  assignee_updated_at=[]\n",
        "  requester_updated_at=[]\n",
        "  status_updated_at=[]\n",
        "  initially_assigned_at=[]\n",
        "  assigned_at=[]\n",
        "  solved_at=[]\n",
        "  latest_comment_added_at=[]\n",
        "  reply_time_in_minutes_calendar=[]\n",
        "  reply_time_in_minutes_business=[]\n",
        "  first_resolution_time_in_minutes_calendar=[]\n",
        "  first_resolution_time_in_minutes_business=[]\n",
        "  full_resolution_time_in_minutes_calendar=[]\n",
        "  full_resolution_time_in_minutes_business=[]\n",
        "  agent_wait_time_in_minutes_calendar=[]\n",
        "  agent_wait_time_in_minutes_business=[]\n",
        "  requester_wait_time_in_minutes_calendar=[]\n",
        "  requester_wait_time_in_minutes_business=[]\n",
        "  on_hold_time_in_minutes_calendar=[]\n",
        "  on_hold_time_in_minutes_business=[]\n",
        "\n",
        "  for index, i in enumerate(df_ticket['metric_set']):\n",
        "    try:\n",
        "      group_stations.append(i['group_stations'])\n",
        "      assignee_stations.append(i['assignee_stations'])\n",
        "      reopens.append(i['reopens'])\n",
        "      replies.append(i['replies'])\n",
        "      assignee_updated_at.append(i['assignee_updated_at'])\n",
        "      requester_updated_at.append(i['requester_updated_at'])\n",
        "      status_updated_at.append(i['status_updated_at'])\n",
        "      initially_assigned_at.append(i['initially_assigned_at'])\n",
        "      assigned_at.append(i['assigned_at'])\n",
        "      solved_at.append(i['solved_at'])\n",
        "      latest_comment_added_at.append(i['latest_comment_added_at'])\n",
        "      reply_time_in_minutes_calendar.append(i['reply_time_in_minutes']['calendar'])\n",
        "      reply_time_in_minutes_business.append(i['reply_time_in_minutes']['business'])\n",
        "      first_resolution_time_in_minutes_calendar.append(i['first_resolution_time_in_minutes']['calendar'])\n",
        "      first_resolution_time_in_minutes_business.append(i['first_resolution_time_in_minutes']['business'])\n",
        "      full_resolution_time_in_minutes_calendar.append(i['full_resolution_time_in_minutes']['calendar'])\n",
        "      full_resolution_time_in_minutes_business.append(i['full_resolution_time_in_minutes']['business'])\n",
        "      agent_wait_time_in_minutes_calendar.append(i['agent_wait_time_in_minutes']['calendar'])\n",
        "      agent_wait_time_in_minutes_business.append(i['agent_wait_time_in_minutes']['business'])\n",
        "      requester_wait_time_in_minutes_calendar.append(i['requester_wait_time_in_minutes']['calendar'])\n",
        "      requester_wait_time_in_minutes_business.append(i['requester_wait_time_in_minutes']['business'])\n",
        "      on_hold_time_in_minutes_calendar.append(i['on_hold_time_in_minutes']['calendar'])\n",
        "      on_hold_time_in_minutes_business.append(i['on_hold_time_in_minutes']['business'])\n",
        "    except:\n",
        "      group_stations.append(None)\n",
        "      assignee_stations.append(None)\n",
        "      reopens.append(None)\n",
        "      replies.append(None)\n",
        "      assignee_updated_at.append(None)\n",
        "      requester_updated_at.append(None)\n",
        "      status_updated_at.append(None)\n",
        "      initially_assigned_at.append(None)\n",
        "      assigned_at.append(None)\n",
        "      solved_at.append(None)\n",
        "      latest_comment_added_at.append(None)\n",
        "      reply_time_in_minutes_calendar.append(None)\n",
        "      reply_time_in_minutes_business.append(None)\n",
        "      first_resolution_time_in_minutes_calendar.append(None)\n",
        "      first_resolution_time_in_minutes_business.append(None)\n",
        "      full_resolution_time_in_minutes_calendar.append(None)\n",
        "      full_resolution_time_in_minutes_business.append(None)\n",
        "      agent_wait_time_in_minutes_calendar.append(None)\n",
        "      agent_wait_time_in_minutes_business.append(None)\n",
        "      requester_wait_time_in_minutes_calendar.append(None)\n",
        "      requester_wait_time_in_minutes_business.append(None)\n",
        "      on_hold_time_in_minutes_calendar.append(None)\n",
        "      on_hold_time_in_minutes_business.append(None)\n",
        "\n",
        "  df_ticket['grupos asignados']=group_stations\n",
        "  df_ticket['agentes asignados']=assignee_stations\n",
        "  df_ticket['reaperturas']=reopens\n",
        "  df_ticket['cantidad de respuestas']=replies\n",
        "  df_ticket['ultima actualizacion agente']=assignee_updated_at\n",
        "  df_ticket['ultima actualizacion solicitante']=requester_updated_at\n",
        "  df_ticket['ultima actualizacion estado']=status_updated_at\n",
        "  df_ticket['inicialmente asignado']=initially_assigned_at\n",
        "  df_ticket['fecha asignacion']=assigned_at\n",
        "  df_ticket['ultimo comentario agregado']=latest_comment_added_at\n",
        "  df_ticket['tiempo respuesta calendario']=reply_time_in_minutes_calendar\n",
        "  df_ticket['tiempo respuesta negocio']=reply_time_in_minutes_business\n",
        "  df_ticket['primera resolucion calendario']=first_resolution_time_in_minutes_calendar\n",
        "  df_ticket['primera resolucion negocio']=first_resolution_time_in_minutes_business\n",
        "  df_ticket['resolucion completa calendario']=full_resolution_time_in_minutes_calendar\n",
        "  df_ticket['resolucion completa negocio']=full_resolution_time_in_minutes_business\n",
        "  df_ticket['espera agente calendario']=agent_wait_time_in_minutes_calendar\n",
        "  df_ticket['espera agente negocio']=agent_wait_time_in_minutes_business\n",
        "  df_ticket['espera solicitante calendario']=requester_wait_time_in_minutes_calendar\n",
        "  df_ticket['espera solicitante negocio']=requester_wait_time_in_minutes_business\n",
        "  df_ticket['tiempo espera calendario']=on_hold_time_in_minutes_calendar\n",
        "  df_ticket['tiempo espera negocio']=on_hold_time_in_minutes_business\n",
        "\n",
        "  return df_ticket, is_cont, end_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pXgTqHB_3Vm"
      },
      "outputs": [],
      "source": [
        "#a,b,c=data_ticket(z_domain,start_time,headers)\n",
        "#a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wsJOrGA7Q1O"
      },
      "outputs": [],
      "source": [
        "def data_user(z_domain,start_time,headers):\n",
        "  continua2=True\n",
        "  continua=True\n",
        "  org_time=str(946684800)\n",
        "  user_time=str(946684800)\n",
        "  all_org={}\n",
        "  iter=1\n",
        "  print(\"---Lo lamentamos, esto puede demorar\")\n",
        "  while continua2:\n",
        "    print(\"---extración de info org iter \"+str(iter)+\",COD \"+org_time)\n",
        "    url2 = z_domain+\"/api/v2/incremental/organizations.json?start_time=\"+org_time\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url2, 'GET', headers=headers)\n",
        "    data2=json.loads(content)\n",
        "    time.sleep(15)\n",
        "    #print(data2)\n",
        "    #print(\"_________\")\n",
        "    for idx, x in enumerate(data2['organizations']):\n",
        "      all_org[str(x['id'])]=[x['name']]\n",
        "\n",
        "    if(data2['end_of_stream']):\n",
        "      continua2=False \n",
        "    org_time=str(data2['end_time'])\n",
        "  print(\"---Se obtuvo la información de las organizaciones\")\n",
        "  #__________________________________________________________________________________________________\n",
        "  all_user={}\n",
        "  iter=1\n",
        "  while continua:\n",
        "    print(\"---extración de info usuario iter \"+str(iter)+\",COD \"+user_time)\n",
        "    url = z_domain+\"/api/v2/incremental/users.json?start_time=\"+user_time\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "    time.sleep(15)\n",
        "    #print(data)\n",
        "    #print(\"_________\")\n",
        "    for idx, x in enumerate(data['users']):\n",
        "      if x['organization_id'] is not None:\n",
        "        nom_org=all_org[str(x['organization_id'])][0]\n",
        "      else:\n",
        "        nom_org=\"\"\n",
        "      all_user[str(x['id'])]=[x['name'],x['email'],str(nom_org)]\n",
        "\n",
        "    if(data['end_of_stream']):\n",
        "      continua=False \n",
        "    user_time=str(data['end_time'])\n",
        "    iter=iter+1\n",
        "  print(\"---Se obtuvo la información de los usuarios\")\n",
        "\n",
        "  return all_user\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CqMreMh-sUk"
      },
      "outputs": [],
      "source": [
        "#a2= data_user(z_domain,start_time,headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agXWTzt1b_7d"
      },
      "outputs": [],
      "source": [
        "#a2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt3VlojwRUZ2"
      },
      "outputs": [],
      "source": [
        "#os.chdir(carpeta_contenedora)\n",
        "#file_extension = '.csv'\n",
        "#all_filenames = [i for i in glob.glob(f\"*{file_extension}\")]\n",
        "#all_filenames\n",
        "#combined_csv_data = pd.concat([pd.read_csv(f, encoding='UTF-8') for f in all_filenames])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOSjbf0tP08y"
      },
      "outputs": [],
      "source": [
        "def generarDataSetCamposTicket(z_domain,start_time,headers):    \n",
        "    url = z_domain+\"/api/v2/ticket_fields\"\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "    campos_de_ticket={}\n",
        "    for idx, x in enumerate(data['ticket_fields']):\n",
        "        id =  data['ticket_fields'][idx]['id']\n",
        "        nombreCampo =  data['ticket_fields'][idx]['title']\n",
        "        type =  data['ticket_fields'][idx]['type']\n",
        "\n",
        "\n",
        "        if type == 'tagger' or type == 'multiselect':\n",
        "            custom_field_options = data['ticket_fields'][idx]['custom_field_options']\n",
        "            opciones_campo = []\n",
        "            for idy, y in enumerate(custom_field_options):\n",
        "                opciones_campo.append([custom_field_options[idy]['raw_name'], custom_field_options[idy]['value']])\n",
        "\n",
        "            campos_de_ticket[id]=[nombreCampo, type, opciones_campo]\n",
        "\n",
        "        if type == 'checkbox':\n",
        "            tag = custom_field_options = data['ticket_fields'][idx]['tag']\n",
        "            campos_de_ticket[id]=[nombreCampo, type, tag]\n",
        "\n",
        "        if type != 'checkbox' and type != 'tagger' and type != 'multiselect':\n",
        "            campos_de_ticket[id]=[nombreCampo, type]\n",
        "\n",
        "    \n",
        "    return campos_de_ticket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM-J95g8cDtq"
      },
      "outputs": [],
      "source": [
        "#lista_campos=generarDataSetCamposTicket(z_domain,start_time,headers)\n",
        "#lista_campos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHcltN5wrkBb"
      },
      "outputs": [],
      "source": [
        "def generarDataSetFormularios(z_domain,start_time,headers):    \n",
        "    url = z_domain+\"/api/v2/ticket_forms\"\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "    time.sleep(4)\n",
        "    formularios={}\n",
        "    for idx, x in enumerate(data['ticket_forms']):\n",
        "        id = data['ticket_forms'][idx]['id'];\n",
        "        nombre = data['ticket_forms'][idx]['name'];\n",
        "        formularios[id] = nombre\n",
        "        \n",
        "    return formularios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOPcy7ur5_af"
      },
      "outputs": [],
      "source": [
        "#generarDataSetFormularios(z_domain,start_time,headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49f8Gma2rpnC"
      },
      "outputs": [],
      "source": [
        "def generarDataSetAgentes():    \n",
        "    url = z_domain+\"/api/v2/users?role=agent&role=admin\"\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "\n",
        "    agentes={}\n",
        "    for idx, x in enumerate(data['users']):\n",
        "        id = data['users'][idx]['id'];\n",
        "        nombre = data['users'][idx]['name'];\n",
        "        agentes[id] = nombre\n",
        "        \n",
        "    return agentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbx5Mwggrtji"
      },
      "outputs": [],
      "source": [
        "def generarDataSetMarcas(z_domain,start_time,headers):    \n",
        "    url = z_domain+\"/api/v2/brands\"\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "    time.sleep(4)\n",
        "    marcas={}\n",
        "    for idx, x in enumerate(data['brands']):\n",
        "        id = data['brands'][idx]['id'];\n",
        "        nombre = data['brands'][idx]['name'];\n",
        "        marcas[id] = nombre\n",
        "        \n",
        "    return marcas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZReuhNcIsYR"
      },
      "outputs": [],
      "source": [
        "def generarDataGrupo(z_domain,start_time,headers):    \n",
        "    url = z_domain+\"/api/v2/groups\"\n",
        "    http = httplib2.Http()\n",
        "    response, content = http.request(url, 'GET', headers=headers)\n",
        "    data=json.loads(content)\n",
        "    time.sleep(4)\n",
        "    grupo={}\n",
        "    for idx, x in enumerate(data['groups']):\n",
        "        id = x['id']\n",
        "        nombre = x['name']\n",
        "        grupo[id] = nombre\n",
        "        \n",
        "    return grupo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOctxr_znIbQ"
      },
      "outputs": [],
      "source": [
        "def etl_ticket(z_domain,start_time,headers,carpeta_contenedora):\n",
        "  cont=True\n",
        "  list_csv=[]\n",
        "  N_1=1\n",
        "  print(\"Estamos extrayendo las referencias para este ETL\")\n",
        "  print(\"-Descargando data formularios\")\n",
        "  lista_formulario=generarDataSetFormularios(z_domain,start_time,headers)\n",
        "  print(\"--Cantidad\",len(lista_formulario))\n",
        "  print(\"-Descargando data usuarios\")\n",
        "  lista_usuario=data_user(z_domain,start_time,headers)\n",
        "  print(\"--Cantidad\",len(lista_usuario))\n",
        "  print(\"-Descargando data campos personalizados\")\n",
        "  lista_campos=generarDataSetCamposTicket(z_domain,start_time,headers)\n",
        "  print(\"--Cantidad de campos\",len(lista_campos))\n",
        "  print(\"-Descargando lista de marcas\")\n",
        "  lista_marca=generarDataSetMarcas(z_domain,start_time,headers)\n",
        "  print(\"--Cantidad\",len(lista_marca))\n",
        "  print(\"-Descargando lista de grupos\")\n",
        "  lista_grupo=generarDataGrupo(z_domain,start_time,headers)\n",
        "  print(\"--Cantidad\",len(lista_grupo))\n",
        "  try:\n",
        "    os.remove(carpeta_contenedora+\"/01.csv\")\n",
        "  except:\n",
        "    print(\"No se encontro un archivo 01.csv\")\n",
        "  print(\"Parte extracción de los datos de tickets\")\n",
        "  while cont:\n",
        "    print(\"_______________________________________________________\")\n",
        "    print(\"-Iteración \"+str(N_1)+\":\",start_time, \"Trabajando 😔\")\n",
        "    data,cont_1,new_start=data_ticket(z_domain,start_time,headers)\n",
        "    print(\"--Se consiguío los datos de Zendesk\")\n",
        "    \n",
        "    time.sleep(4)\n",
        "    # Acá parte la sincronización del nombre de formulario\n",
        "    nombre_form=[]\n",
        "    for ticket in data['ticket_form_id']:\n",
        "      if np.isnan(ticket):\n",
        "        nombre_form.append(\"\")\n",
        "      else:\n",
        "        nombre_form.append(lista_formulario[ticket])\n",
        "\n",
        "    data['Nombre del formulario']=nombre_form\n",
        "    # Información de los usuarios\n",
        "    \n",
        "    nombre_solicitante=[]\n",
        "    correo_solicitante=[]\n",
        "    organizacion_solicitante=[]\n",
        "    nombre_agente=[]\n",
        "    correo_agente=[]\n",
        "    organizacion_agente=[]\n",
        "    nombre_remitente=[]\n",
        "    correo_remitente=[]\n",
        "    organizacion_remitente=[]\n",
        "\n",
        "    #solicitante_____________________________________________________\n",
        "    for req in data['requester_id']:\n",
        "      if np.isnan(req):\n",
        "        nombre_solicitante.append(\"\")\n",
        "        correo_solicitante.append(\"\")\n",
        "        organizacion_solicitante.append(\"\")\n",
        "      else:\n",
        "        try:\n",
        "          nombre_solicitante.append(lista_usuario[str(int(req))][0])\n",
        "          correo_solicitante.append(lista_usuario[str(int(req))][1])\n",
        "          organizacion_solicitante.append(lista_usuario[str(int(req))][2])\n",
        "        except:\n",
        "          nombre_solicitante.append(\"\")\n",
        "          correo_solicitante.append(\"\")\n",
        "          organizacion_solicitante.append(\"\")\n",
        "\n",
        "    #Agente_____________________________________________________\n",
        "    for agent in data['assignee_id']:\n",
        "      if np.isnan(agent):\n",
        "        nombre_agente.append(\"\")\n",
        "        correo_agente.append(\"\")\n",
        "        organizacion_agente.append(\"\")\n",
        "      else:\n",
        "        try:\n",
        "          nombre_agente.append(lista_usuario[str(int(agent))][0])\n",
        "          correo_agente.append(lista_usuario[str(int(agent))][1])\n",
        "          organizacion_agente.append(lista_usuario[str(int(agent))][2])\n",
        "        except:\n",
        "          nombre_agente.append(\"\")\n",
        "          correo_agente.append(\"\")\n",
        "          organizacion_agente.append(\"\")\n",
        "\n",
        "    #remitente_____________________________________________________\n",
        "    for submi in data['submitter_id']:\n",
        "        if np.isnan(submi):\n",
        "          nombre_remitente.append(\"\")\n",
        "          correo_remitente.append(\"\")\n",
        "          organizacion_remitente.append(\"\")\n",
        "        else:\n",
        "          try:\n",
        "            nombre_remitente.append(lista_usuario[str(int(submi))][0])\n",
        "            correo_remitente.append(lista_usuario[str(int(submi))][1])\n",
        "            organizacion_remitente.append(lista_usuario[str(int(submi))][2])\n",
        "          except:\n",
        "            nombre_remitente.append(\"\")\n",
        "            correo_remitente.append(\"\")\n",
        "            organizacion_remitente.append(\"\")\n",
        "    \n",
        "\n",
        "\n",
        "    data['Nombre del solicitante']=nombre_solicitante\n",
        "    data['Correo del solicitante']=correo_solicitante\n",
        "    data['Organización del solicitante']=organizacion_solicitante\n",
        "\n",
        "    data['Nombre del agente']=nombre_agente\n",
        "    data['Correo del agente']=correo_agente\n",
        "    data['Organización del agente']=organizacion_agente\n",
        "\n",
        "    data['Nombre del remitente']=nombre_remitente\n",
        "    data['Correo del remitente']=correo_remitente\n",
        "    data['Organización del remitente']=organizacion_remitente\n",
        "\n",
        "    # conversión de las columnas\n",
        "    cl=data.columns.to_list()\n",
        "    \n",
        "    for col in cl:\n",
        "      try:\n",
        "        if int(col):\n",
        "          field=lista_campos[int(col)]\n",
        "          if field[1]==\"tagger\":\n",
        "            for val in field[2]:\n",
        "              data[col]=data[col].replace(val[1],val[0])\n",
        "          elif field[1]==\"multiselect\":\n",
        "            for val in field[2]:\n",
        "              data[col]=data[col].replace(val[1],val[0])\n",
        "          elif field[1]==\"checkbox\":\n",
        "            data[col]=data[col].replace(None,False)\n",
        "          else:\n",
        "            True\n",
        "          data.rename(columns = {col:field[0]}, inplace = True)\n",
        "      except:\n",
        "        True\n",
        "    brands=[]\n",
        "    for brand in data['brand_id']:\n",
        "      try:\n",
        "        brands.append(lista_marca[int(brand)])\n",
        "      except:\n",
        "        brands.append(\"\")\n",
        "    data['marca']=brands\n",
        "\n",
        "    #se agrega grupo de agente asignado\n",
        "    assig_group=[]\n",
        "    for group in data['group_id']:\n",
        "      try:\n",
        "        assig_group.append(lista_grupo[int(group)])\n",
        "      except:\n",
        "        assig_group.append(\"\")\n",
        "    data['grupo agente']=assig_group\n",
        "    #Se elimina las columnas innecesarias\n",
        "    print(\"--Estoy eliminando columnas innecesarias 😩\")\n",
        "    data=data.drop(columns=['via',\"forum_topic_id\",\"custom_fields\",\"fields\",\"index\"])\n",
        "    #Estoy actualizando los nombres de las columnas\n",
        "    print(\"--Cambio el nombre de las columnas 🤤\")\n",
        "    data.rename(columns = {\"type\":\"tipo\",\"subject\":\"asunto\",\"description\":\"descripcion\",\"priority\":\"prioridad\",\"status\":\"estado\",\"recipient\":\"casilla receptora\",\"requester_id\":\"id solicitante\",\"submitter_id\":\"id remitente\",\"assignee_id\":\"id agente asignado\",\"organization_id\":\"id organizacion solicitante\",\"group_id\":\"id grupo asignado\",\"ticket_form_id\":\"id formulario\"}, inplace = True)\n",
        "    #Crear archivo\n",
        "    data.to_csv(carpeta_contenedora+'/'+start_time+\".csv\")\n",
        "    list_csv.append(start_time+\".csv\")\n",
        "    \n",
        "    if(cont_1):\n",
        "      cont=False\n",
        "    start_time=str(new_start)\n",
        "    N_1=N_1+1\n",
        "    print(\"--Lista la transformación 😉\")\n",
        "\n",
        "  \n",
        "  os.chdir(carpeta_contenedora)\n",
        "  file_extension = '.csv'\n",
        "  all_filenames = [i for i in glob.glob(f\"*{file_extension}\")]\n",
        "  combined_csv_data = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
        "\n",
        "\n",
        "  \n",
        "  combined_csv_data = combined_csv_data.loc[:, ~combined_csv_data.columns.str.startswith('Unnamed')]\n",
        "  combined_csv_data.to_excel(\"01.xlsx\")\n",
        "  for file in list_csv:\n",
        "   os.remove(carpeta_contenedora+\"/\"+file)\n",
        "  print(\"Ya se creo el archivo 01.xlsx 🤤\")\n",
        "\n",
        "  return combined_csv_data\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOgLqdgM6iSk"
      },
      "outputs": [],
      "source": [
        "df_etl=etl_ticket(z_domain,start_time,headers,carpeta_contenedora)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCJZMUFOH_EG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi2-AroahTCt"
      },
      "outputs": [],
      "source": [
        "df_etl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az3hc_urbO37"
      },
      "outputs": [],
      "source": [
        "a,b,c=data_ticket(z_domain,\"1659916800\",headers)\n",
        "lista_campos=generarDataSetCamposTicket(z_domain,start_time,headers)\n",
        "lista_usuario=data_user(z_domain,start_time,headers)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnsMXXTudwxz"
      },
      "outputs": [],
      "source": [
        "combined_csv_data=a\n",
        "\n",
        "lista_usuario=data_user(z_domain,start_time,headers)\n",
        "nombre_solicitante=[]\n",
        "correo_solicitante=[]\n",
        "organizacion_solicitante=[]\n",
        "nombre_agente=[]\n",
        "correo_agente=[]\n",
        "organizacion_agente=[]\n",
        "nombre_remitente=[]\n",
        "correo_remitente=[]\n",
        "organizacion_remitente=[]\n",
        "\n",
        "#solicitante_____________________________________________________\n",
        "for req in combined_csv_data['requester_id']:\n",
        "  if np.isnan(req):\n",
        "    nombre_solicitante.append(\"\")\n",
        "    correo_solicitante.append(\"\")\n",
        "    organizacion_solicitante.append(\"\")\n",
        "  else:\n",
        "    try:\n",
        "      nombre_solicitante.append(lista_usuario[str(req)][0])\n",
        "      correo_solicitante.append(lista_usuario[str(req)][1])\n",
        "      organizacion_solicitante.append(lista_usuario[str(req)][2])\n",
        "    except:\n",
        "      nombre_solicitante.append(\"\")\n",
        "      correo_solicitante.append(\"\")\n",
        "      organizacion_solicitante.append(\"\")\n",
        "\n",
        "#Agente_____________________________________________________\n",
        "for agent in combined_csv_data['assignee_id']:\n",
        "  if np.isnan(agent):\n",
        "    nombre_agente.append(\"\")\n",
        "    correo_agente.append(\"\")\n",
        "    organizacion_agente.append(\"\")\n",
        "  else:\n",
        "    try:\n",
        "      nombre_agente.append(lista_usuario[str(agent)][0])\n",
        "      correo_agente.append(lista_usuario[str(agent)][1])\n",
        "      organizacion_agente.append(lista_usuario[str(agent)][2])\n",
        "    except:\n",
        "      nombre_agente.append(\"\")\n",
        "      correo_agente.append(\"\")\n",
        "      organizacion_agente.append(\"\")\n",
        "\n",
        "#remitente_____________________________________________________\n",
        "for submi in combined_csv_data['submitter_id']:\n",
        "    if np.isnan(submi):\n",
        "      nombre_remitente.append(\"\")\n",
        "      correo_remitente.append(\"\")\n",
        "      organizacion_remitente.append(\"\")\n",
        "    else:\n",
        "      try:\n",
        "        nombre_remitente.append(lista_usuario[str(submi)][0])\n",
        "        correo_remitente.append(lista_usuario[str(submi)][1])\n",
        "        organizacion_remitente.append(lista_usuario[str(submi)][2])\n",
        "      except:\n",
        "        nombre_remitente.append(\"\")\n",
        "        correo_remitente.append(\"\")\n",
        "        organizacion_remitente.append(\"\")\n",
        "\n",
        "\n",
        "combined_csv_data['Nombre del solicitante']=nombre_solicitante\n",
        "combined_csv_data['Correo del solicitante']=correo_solicitante\n",
        "combined_csv_data['Organización del solicitante']=organizacion_solicitante\n",
        "\n",
        "combined_csv_data['Nombre del agente']=nombre_agente\n",
        "combined_csv_data['Correo del agente']=correo_agente\n",
        "combined_csv_data['Organización del agente']=organizacion_agente\n",
        "\n",
        "combined_csv_data['Nombre del remitente']=nombre_remitente\n",
        "combined_csv_data['Correo del remitente']=correo_remitente\n",
        "combined_csv_data['Organización del remitente']=organizacion_remitente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOfZ2x4orYn0"
      },
      "outputs": [],
      "source": [
        "lista_usuario[\"423691799452\"][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xAefjwYPMFv"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}